import numpy as np

'''Ğ ĞµĞ°Ğ»Ğ¸Ğ·ÑƒĞ¹Ñ‚Ğµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑĞ²ÑĞ·Ğ½ÑƒÑ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½ÑƒÑ ÑĞµÑ‚ÑŒ. ĞĞ° Ğ²Ñ…Ğ¾Ğ´ ÑĞµÑ‚ÑŒ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñƒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ ğ‘Ã—ğ· ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², 
Ğ³Ğ´Ğµ ğ‘ â€“ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ğ¿Ğ°ĞºĞµÑ‚Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ğ· â€“ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ Ğ¸Ğ· ğ‘ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ², Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. 
Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°Ñ‚ÑŒ ÑĞµÑ‚ÑŒ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ğ° Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñƒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° ğ‘Ã—Ğ¡, Ğ³Ğ´Ğµ ğ¶ â€“ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ°. 
ĞĞ° Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¼Ğ¾Ğ¶ĞµÑ‚Ğµ Ğ²Ğ·ÑÑ‚ÑŒ Ğ´Ğ»Ñ ğ¶ Ğ»ÑĞ±Ğ¾Ğµ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğµ Ñ‡Ğ¸ÑĞ»Ğ¾ (5-10 ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²).

Ğ¡ĞµÑ‚ÑŒ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ‚ÑŒ Ğ² ÑĞµĞ±Ñ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ„Ñƒï¿½import numpy as np

Ğ ĞµĞ°Ğ»Ğ¸Ğ·ÑƒĞ¹Ñ‚Ğµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑĞ²ÑĞ·Ğ½ÑƒÑ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½ÑƒÑ ÑĞµÑ‚ÑŒ. ĞĞ° Ğ²Ñ…Ğ¾Ğ´ ÑĞµÑ‚ÑŒ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñƒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ ğ‘Ã—ğ· ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², 
Ğ³Ğ´Ğµ ğ‘ â€“ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ğ¿Ğ°ĞºĞµÑ‚Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ğ· â€“ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ Ğ¸Ğ· ğ‘ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ², Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. 
Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°Ñ‚ÑŒ ÑĞµÑ‚ÑŒ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ğ° Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñƒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° ğ‘Ã—Ğ¡, Ğ³Ğ´Ğµ ğ¶ â€“ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ°. 
ĞĞ° Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¼Ğ¾Ğ¶ĞµÑ‚Ğµ Ğ²Ğ·ÑÑ‚ÑŒ Ğ´Ğ»Ñ ğ¶ Ğ»ÑĞ±Ğ¾Ğµ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğµ Ñ‡Ğ¸ÑĞ»Ğ¾ (5-10 ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²).

Ğ¡ĞµÑ‚ÑŒ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ‚ÑŒ Ğ² ÑĞµĞ±Ñ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸: ReLU â€“ Ğ¾Ğ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾, Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ½Ğ° Ğ²Ñ‹Ğ±Ğ¾Ñ€.

Ğ¡ĞµÑ‚ÑŒ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ğ° Ğ±Ñ‹Ñ‚ÑŒ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ° Ñ‚Ğ°ĞºĞ¸Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ñ‚Ğ¸Ğ¿ ÑĞ»Ğ¾Ñ, ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑĞ»Ğ¾ĞµĞ² Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ±Ñ‹Ğ»Ğ¾ Ğ·Ğ°Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ. 
Ğ’ ÑÑ‚Ğ¾Ğ¼ Ğ²Ğ°Ğ¼ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‡ÑŒ Ğ½Ğ°ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. ĞĞ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Ğ²Ñ‹ Ğ¼Ğ¾Ğ¶ĞµÑ‚Ğµ ÑĞ´ĞµĞ»Ğ°Ñ‚ÑŒ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ»Ğ°ÑÑ (Ğ¸Ğ»Ğ¸ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¹ ĞºĞ»Ğ°ÑÑ, Ğ¸Ğ»Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ, ÑÑ‚Ğ¾ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ ÑĞ·Ñ‹ĞºĞ°, Ğ½Ğ° ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ²Ñ‹ Ğ¿Ğ¸ÑˆĞµÑ‚Ğµ) 
Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ²ÑĞµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹, ÑĞ»Ğ¾Ğ¸ Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ°Ğº Ğ½Ğ°ÑĞ»ĞµĞ´Ğ½Ğ¸ĞºĞ¸ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ĞºĞ»Ğ°ÑÑĞ°.'''

N = 2
D = 2


def ReLU(x):
    return np.maximum(x, 0)


def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_prime(z):# ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ½Ğ°Ñ ÑĞ¸Ğ³Ğ¼Ğ¾Ğ¸Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸
    return sigmoid(z)*(1-sigmoid(z))

def ReLU(z):
    return np.maximum(0,z)

def ReLU_prime(z):
    z = np.where(z > 0, z, 1)
    return np.where(z <= 0, z, 0)

def sigpriozv(x):
    return np.exp(-x)/np.square(1 + np.exp(x))

def nonef_prime(x):
    return x;


def x_funk(x):
    return x


class Network:
    def __init__(self, sizes, activation_func):
        self.sizes = sizes
        self.count_input = sizes[0]
        self.count_output = sizes[-1]
        self.functions = activation_func
        self.num_layers = len(sizes)  # Ñ‡Ğ¸ÑĞ»Ğ¾ ÑĞ»Ğ¾ĞµĞ²
        self.learning_speed = 1000
        self.loss_func = None
        self.biases = [np.random.randn(1, y) for y in sizes[1:]]
        self.weights = [np.random.randn(y+1, x) for x, y in zip(sizes[1:], sizes[:-1])]

    def feedforward(self, a):
        for w, b, func in zip(self.weights, self.biases, self.functions):
            b = np.ones((a.shape[0], 1))
            a = np.concatenate((a, b), axis=1)
            a = func(a.dot(w)+b)
        return a

    def backward(self, a, y):
        for i in range(self.learning_speed):
            a_without_b_save = []
            a_with_b_save = []
            a_with_b_multiplied_w_save = []
            a_after_funk_save = []
            # grads_w = [np.zeros(w.shape) for w in self.weights]

            for w, b, func in zip(self.weights, self.biases, self.functions):

                a_without_b_save.append(a)
                b = np.ones((a.shape[0], 1))
                a = np.concatenate((a, b), axis=1)
                a_with_b_save.append(a)
                a_with_b_multiplied_w_save.append(a.dot(w))
                a = func(a.dot(w))
                a_after_funk_save.append(a)

            y_pred = a
            loss = np.square(y_pred - y)
            y_pred = a_after_funk_save[-1] = loss
            is_first = True
            # for i in range(1, len(self.sizes)):
            #     f_p = funk_prime(a_with_b_multiplied_w_save[-i])

            for w, funk_prime, save, a_after_funk, a_with_b_w in zip(self.weights[::-1], [ReLU_prime, nonef_prime][::-1]
                    , a_with_b_save[::-1], a_after_funk_save[::-1], a_with_b_multiplied_w_save[::-1]):
                f_p = funk_prime(a_with_b_w)
                y_pred = funk_prime(y_pred)
                if is_first:
                    is_first = False
                else:
                    y_pred = np.delete(y_pred, (y_pred.shape[1]-1), axis=1)
                delta_w = (save.T.dot(y_pred))
                w -= delta_w / self.learning_speed
                y_pred = y_pred.dot(w.T)





def rndGenerate(N, D):
    return np.random.rand(N, D)


if __name__ == "__main__":
    matrix = np.random.rand(N,D)

    nw = Network([2, 6, 5], [sigmoid, x_funk])
    nw.feedforward(matrix)
